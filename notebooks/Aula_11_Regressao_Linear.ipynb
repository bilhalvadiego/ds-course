{"cells":[{"metadata":{"id":"U36sTFjnvCSL","slideshow":{"slide_type":"skip"}},"cell_type":"markdown","source":"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/billy-ds/ds-course.git/prod?filepath=notebooks%2FAula_11_Regressao_Linear.ipynb)\n<br>\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/billy-ds/ds-course/blob/prod/notebooks/Aula_11_Regressao_Linear.ipynb)"},{"metadata":{"id":"DbmSKbdzvCSP","slideshow":{"slide_type":"skip"}},"cell_type":"markdown","source":"<a id='section-zero'></a>"},{"metadata":{"id":"k3SauwW1vCSP","slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# Regressão Linear\n\n**Objetivos**: Aprender o que é Regressão Linear e introduzir intuições sobre o Método do Gradiente e o Método do Gradiente Estocástico assim como os problemas de regressão de aprendizagem de máquina. Apresentar a biblioteca `SciKit-Learn`."},{"metadata":{"id":"OtVKi7WNvCSQ","slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"## Definição - Regressão Linear\n\n> Uma regressão linear faz uma predição simplesmente computando uma soma ponderada dos atributos (*features*), mais uma constante chamada viés (*bias*), também chamado de constante (*intercept*)."},{"metadata":{"id":"NPr0AlxBvCSQ","slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"<img src=\"https://github.com/billy-ds/ds-course/blob/prod/notebooks/images/reg-linear.png?raw=1\" alt=\"reg-linear\" style=\"width: 400px;\"/>"},{"metadata":{"id":"jtZGzj-yvCSR","slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"$$ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n$$\n\n$\\hat{y}$ - valor previsto\n\n$\\theta$ - parâmetro do modelo\n\n$n$ - número de atributos (*features*)\n\n$x_i$ - o valor do *inésimo* atributo (*feature*)"},{"metadata":{"id":"6YYrbso_vCSR","slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"### Exemplo\n\n$$\\mathrm{preço~de~residência} = 4500 + 1000\\times \\mathrm{quartos} + 120 \\times \\mathrm{m}^2 + 3000 \\times \\mathrm{banheiros} - 1500 \\times \\mathrm{distância~do~centro~km}$$"},{"metadata":{"id":"3ofzGSfvvCSS","slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Métricas de Desempenho de uma Regressão\n\n<img src=\"https://github.com/billy-ds/ds-course/blob/prod/notebooks/images/erro-reg.png?raw=1\" alt=\"erro-reg\" style=\"width: 400px;\"/>"},{"metadata":{"id":"vo-r2M6QvCST","slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"### *Mean Absolute Error* (MAE) - Erro Absoluto Médio"},{"metadata":{"id":"dj3p0mVIvCST","slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"O erro médio absoluto, MAE (da sigla em inglês Mean Absolute Error), é calculado a partir da **média dos erros absolutos**, ou seja, utilizamos o módulo de cada erro para evitar a subestimação, isso porque, o valor é menos afetado por pontos especialmente extremos (outliers).\n\nCada erro, pode ser interpretado como a diferença entre Y e Ŷ e assim, temos:\n\n$$MAE = \\frac{1}{m}\\Sigma_{i=1}^{m}{|\\hat{y}_i - y_i|}$$\n\nUtilizamos essa medida em séries temporais, pois há casos em que o erro negativo pode zerar o positivo ou dar uma ideia de que o modelo é preciso. Mas aqui, medimos apenas a distância do valor real, independente de ser acima ou abaixo."},{"metadata":{"id":"fQ8ayXd3vCSU","slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"### *Mean Squared Error* (MSE) - Erro Quadrático Médio"},{"metadata":{"id":"-vYrXqhkvCSU","slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"O erro quadrático médio, MSE (da sigla em inglês Mean Squared Error), é comumente usado para verificar a **acurácia de modelos** e dá um maior peso aos maiores erros, já que, ao ser calculado, cada erro é elevado ao quadrado individualmente e, após isso, a média desses erros quadráticos é calculada.\n\nUsando o mesmo conceito de erro utilizado anteriormente, temos a equação abaixo:\n\n$$MSE = \\frac{1}{m}\\Sigma_{i=1}^{m}{(\\hat{y}_i - y_i)^2}$$\n\nPor conta do expoente ao quadrado que o erro assume, essa métrica é bastante sensível a outliers (valores discrepantes) e, caso tenha muitos erros significativos em sua análise, essa métrica poderá ser extrapolada."},{"metadata":{"id":"d2-991xXvCSV","slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"<img src=\"https://github.com/billy-ds/ds-course/blob/prod/notebooks/images/gradient-descent.gif?raw=1\" alt=\"gradient-descent-animation\" style=\"width: 60%;\"/>"},{"metadata":{"id":"D33GfybxvCSV","slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"<img src=\"https://github.com/billy-ds/ds-course/blob/prod/notebooks/images/gradient-descent-2.gif?raw=1\" alt=\"gradient-descent-animation\" style=\"width: 60%;\"/>"},{"metadata":{"id":"Q3zlvvI7vCSV","slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Exemplo com o dataset [Boston House Prices](https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-house-prices-dataset)\n\n\n* $N = 506$\n* Atributos: 13\n    * `CRIM` crime per capita da região\n    * `ZN` proporção de terra residencial\n    * `INDUS` proporção terra comercial não-varejista\n    * `CHAS` *Dummy* se fica as margens do Charles River (1 ou 0)\n    * `NOX` concentração de óxido nítrico (partes por 10 milhões)\n    * `RM` número de quartos\n    * `AGE` idade da residência\n    * `DIS` distância dos cinco centros de emprego de Boston\n    * `RAD` acessibilidade às rodovias radiais\n    * `TAX` valor do IPTU por 10,000 USD\n    * `PTRATIO` relação professor-aluno (*pupil-teacher ratio*) da região\n    * `B` proporção de afro-descendentes na região\n    * `LSTAT` porcentagem de população de baixa-renda\n* **Variável resposta**: valor da casa por 10,000 USD"},{"metadata":{"id":"E2SqHdKevCSW","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"from sklearn.datasets import load_boston\n\nboston = load_boston()\nX = boston['data']\ny = boston['target']","execution_count":null,"outputs":[]},{"metadata":{"id":"kguBQii9vCSY","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"print(f\"Nomes dos Atributos: {boston['feature_names']}\")\nprint(f\"Tamanho de X: {X.shape}\")\nprint(f\"Tamanho de y: {y.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"B5bcuxO7vCSZ","slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"### Quebrando dataset em `train` e `test`\n\nUsar a função do Scikit-Learn [`sklearn.model_selection.train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n\n#### Argumentos:\n\n* matriz a ser dividida - `X` ou `y`\n* `test_size` - `float` ou `int` do tamanho do dataset de teste (padrão $0.25$)\n* `train_size` - padrão `1 - test_size`\n* `random_state` - `int` - seed do gerador de número randômicos (replicabilidade)"},{"metadata":{"id":"eG0HwS_gvCSZ","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.25,\n                                                    random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"id":"0HKATJlevCSZ","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"print(f\"Tamanho de X_train: {X_train.shape}\")\nprint(f\"Tamanho de X_test: {X_test.shape}\")\nprint(f\"Tamanho de y_train: {y_train.shape}\")\nprint(f\"Tamanho de y_test: {y_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"7kdNtrkCvCSa","slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"### Regressão Linear\nUsar o estimador do Scikit-Learn [`sklearn.linear_model.LinearRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n\n#### Retorna:\n* Objeto `estimator` do Scikit-Learn"},{"metadata":{"id":"0r1oagpTvCSa","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nclf = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"id":"WHNzVyzuvCSa","slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"### Classe `Estimators`\n\n* `.fit()` - Treina o Modelo\n    * `X`\n    * `y`\n* `.predict()` - Gera predições do modelo\n    * `X`\n* `.coef_` - Retorna os coeficientes do modelo ($\\theta_i$)\n* `.intercept_` - Retorna o viés/constante (*bias/intercept*) do modelo ($\\theta_0$)"},{"metadata":{"id":"TAu7Kn2HvCSa","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"TqcUfzYtvCSb","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"clf.coef_.tolist()","execution_count":null,"outputs":[]},{"metadata":{"id":"nX4dmvcXvCSb","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"# Coeficientes do modelo\nfor feature, coef in zip(boston['feature_names'].tolist(), clf.coef_.tolist()):\n    print(f\"{feature}: {round(coef, 2)}\")\n\n# Constante do modelo\nprint(f\"Constante: {round(clf.intercept_, 2)}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"38_TQQ-fvCSb","slideshow":{"slide_type":"subslide"}},"cell_type":"markdown","source":"### Erro do Modelo\n"},{"metadata":{"id":"tvV9Uq3pvCSb","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ny_pred = clf.predict(X_test)\n\nprint(f\"MSE de Teste: {mean_squared_error(y_test, y_pred):1.1f}\")\nprint(f\"MAE de Teste: {mean_absolute_error(y_test, y_pred):1.1f}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"rtZNSFEwvw3M","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nr_quadrado = r2_score(y_test, y_pred)\n\nprint('O R² é de {0}. Isto é, {1}% dos casos são explicados pelo modelo.'.format(round(r_quadrado,3), round(r_quadrado*100,1)))","execution_count":null,"outputs":[]},{"metadata":{"id":"4Sl_T3S5vCSc","slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Atividade - Regressão com o dataset [Diabetes](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)\n\n* $N = 442$\n* Atributos: 10\n    * `age`\n    * `sex`\n    * `bmi` Índice de Massa Corpórea (IMC) - *Body Mass Index* (BMI)\n    * `bp` pressão arterial média *blood pressure* (bp)\n    * `s1` colesterol total\n    * `s2` colesterol LDL\n    * `s3` colesterol HDL\n    * `s4` colesterol VLDL\n    * `s5` triglicerides\n    * `s6` glicose\n* Variável dependente: medida quantitativa de progressão da diabetes\n\n* Rodem o `LinearRegression()` nos dados de treino e mensure o desempenho nos dados de teste.\n\n>Obs: usar `test_size = 0.25` e `random_state = 123`"},{"metadata":{"id":"9T60YI52vCSc","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"from sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\nX = diabetes['data']\ny = diabetes['target']","execution_count":null,"outputs":[]},{"metadata":{"id":"t3840QwGvCSc","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"print(f\"Nomes dos Atributos: {diabetes['feature_names']}\")\nprint(f\"Tamanho de X: {X.shape}\")\nprint(f\"Tamanho de y: {y.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"CUruN1XJvCSd","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"id":"6QRCdTnuvCSd","trusted":false,"slideshow":{"slide_type":"subslide"}},"cell_type":"code","source":"print(f\"Tamanho de X_train: {X_train.shape}\")\nprint(f\"Tamanho de X_test: {X_test.shape}\")\nprint(f\"Tamanho de y_train: {y_train.shape}\")\nprint(f\"Tamanho de y_test: {y_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"9b9Uf0yFvCSd","slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"**Tópico Anterior:** [Introdução ao ML](https://mybinder.org/v2/gh/billy-ds/ds-course.git/prod?filepath=notebooks%2FAula_10_Machine_Learning.ipynb) | **Ir para o** [Início](#section-zero) | **Próximo Tópico:** [Regressão Logística](https://mybinder.org/v2/gh/billy-ds/ds-course.git/prod?filepath=notebooks%2FAula_12_Regressao_Logistica.ipynb)"}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"rise":{"autolaunch":true,"enable_chalkboard":true,"progress":true,"scroll":true,"slideNumber":true,"theme":"white"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":1}